#+TITLE: What is AI?
#+AUTHOR: Marcus Birkenkrahe
#+Subtitle: Seminar on Artificial Intelligence
#+STARTUP: hideblocks
#+reveal_theme: black
#+reveal_init_options: transition:'cube'
#+OPTIONS: toc:nil num:nil ^:nil
#+INFOJS_OPT: :view:info
* What're you going to learn?

  * What is intelligence?
  * Different approaches to AI
  * The standard model of AI
  * Bounded rationality
  * The Value alignment problem
  * Asimov's Robot Laws
  * What's next?

* What is intelligence?

  #+attr_html: :height 500px
  [[./img/intelligence.gif]]

** Search patterns

   #+attr_html: :width 600px
   [[./img/googletrends.png]]

** Group work

   #+attr_html: :height 200px
   [[./img/groupwork.gif]]

   * Get together in groups of 2-3
   * Define INTELLIGENCE (5')
   * Define ARTIFICIAL INTELLIGENCE (5')
   * Briefly present your results (10')

* Different approaches to AI

  #+attr_html: :height 300px
  [[./img/fields.gif]]

  Which fields of inquiry (= disciplines) to use?

** Fields of systematic inquiry

   #+attr_html: :height 200px
   [[./img/fields.gif]]

   * Language
   * Philosophy
   * Science
   * History

** Fundamental questions

   #+attr_html: :width 500px
   [[./img/humanmachine.jpg]]

   * Should we focus on humans?
   * Should we focus on machines?

** Four approaches

   |               | THOUGHT / LOGIC      | BEHAVIOR / ACTION   |
   |---------------+----------------------+---------------------|
   | HUMANITY      | Cognitive modeling   | Turing Test         |
   | RATIONALITY   | Laws of Thought      | Rational Agents     |

*** Four scenarios

    #+attr_html: :height 500px
    [[./img/approaches1.png]]

*** Acting humanly ("Turing test" approach)

    | Natural language processing |
    | Knowledge representation    |
    | Automated reasoning         |
    | Machine learning            |
    | Computer vision             |
    | Robotics                    |

*** Thinking humanly ("cognitive modeling" approach)

    |  Introspection              |
    |  Psychological experiments  |
    |  Brain imaging              |
    |  Cognitive science          |
    |  Algorithms                 |

*** Thinking rationally ("laws of thought" approach)

    |  Syllogistic reasoning  |
    |  Logic                  |
    |  Expert systems         |
    |  Uncertainty            |
    |  Probability            |

*** Acting rationally ("rational agent" approach)

    |  Combination approach   |
    |  Constructivist         |
    |  Doing the right thing  |
    |  Standard model         |
    |  Control theory         |

** Major issues

   #+attr_html: :height 200px
   [[./img/issues.gif]]

   * Bounded Rationality
   * Value alignment problem

** Bounded rationality

   #+attr_html: :width 300px
   [[./img/bakopoulos.png]]

   Image: [[bakopoulos1985][Bakopoulos, 1985]]

   #+begin_quote
   AIMA: "For perfect rationality, the computational demands are just
   too high."
   #+end_quote

   #+begin_notes
   The article by [[bakopoulos1985][Bakopoulos (1985)]] helped to move IT from an arcane
   discipline for technologists and nerds to a mainstream service
   industry. To do this, Bakopoulos capitalized on the notion that
   humans are not perfectly rational, that their rationality is
   bounded by their humanity: "Data has no value unless put in the
   context of the appropriate models, a process taxing the human
   capacities to communicate, memorize and process information, and
   thus leading to /bounded rationality/, which is a central concept
   in organizational behavior theory."

   I worked through this article a while back for a lecture I was
   preparing, and found it remarkably difficult for such an old paper,
   with a lot of connections in different directions - technology,
   business, information theory, and philosophy.

   Bakopoulos' result that information systems have no (or only
   little) value without being properly integrated into business, and
   that IT is useless if it does not 'speak" to people in ways that
   they can understand and with results that they can measure, is
   wonderfully relevant for the future development of AI, too.
   
   #+end_notes

** Value alignment

   #+attr_html: :width 200px
   [[./img/mechanicalturk.png]]

   Image: [[https://www.amazon.com/Turk-Famous-Eighteenth-Century-Chess-Playing-Machine/dp/B000HWZ28Q][The Mechanical Turk]]

   #+begin_quote
   AIMA: "The values or objectives put into the machine must be
   aligned with those of the human."
   #+end_quote

   #+begin_notes
   This kind of "alignment" sounds like an engineering task, but
   actually it is a lot more complicated (or actually complex in a
   technical sense): if rational agents are supposed to "do the right
   thing", then their actions have to be not just logical, but
   appropriate and moral. There is no algorithm for that - both
   appropriateness (e.g. to a situation) and morality depend on the
   circumstances. Take the example of Grace, the "ultra-lifelike
   robotic nurse" from Hanson Robotics: she was designed for a certain
   set of circumstances and both by, and for a particular set of moral
   values. Will she do equally well in Japan and in Belgium, the
   country with the "[[https://www.pbs.org/newshour/show/right-die-belgium-inside-worlds-liberal-euthanasia-laws][world's most liberal euthanasia law]]", and
   therefore possibly a different approach to caring for the elderly?

   The example of an 'unethical rational agent' chosen by AIMA, which
   relates to the "Mechanical Turk", a chess machine that was operated
   by a human hidden inside the machine, is admittedly a lot less
   critical than when taking care of the sick and elderly is at
   stake. However, the age of AI that dazzled us by beating chess
   champions is long behind us, and the age of robots like Grace is
   upon us!
   #+end_notes

** Pros and cons

   #+attr_html: :height 200px
   [[./img/groupwork.gif]]

   * Get together in groups of 2-3
   * Each group covers one approach
   * List pros and cons of your approach
   * Put your results [[https://ideaboardz.com/for/AI%20approaches%20pros%20&amp;%20cons/4063343][on the Kanban board]]

** [[https://en.wikipedia.org/wiki/Three_Laws_of_Robotics][Asimov's robot laws]]

   #+attr_html: :height 400px
   [[./img/asimov.jpg]]

   Image: cover of "I, Robot" by Isaac Asimov (1940)

*** Which approach fits these laws best?

    1) A robot may not injure a human being or, through inaction, allow
       a human being to come to harm.
    2) A robot must obey the orders given it by human beings except
       where such orders would conflict with the First Law.
    3) A robot must protect its own existence as long as such
       protection does not conflict with the First or Second Law.

* What's next?

  #+attr_html: :height 300px
  [[./img/river.gif]]

  * Scientific foundations of AI
  * History of AI

* Any questions?

  #+attr_html: :height 400px
  [[./img/thankyou.gif]]

  [[https://github.com/birkenkrahe/ai482/tree/main/2_what_is_ai][This presentation is available online.]]

* References

  <<bakopoulos1985>> Bakopoulos, J. Yannis, "Toward a More Precise
  Concept of Information Technology" (1985). ICIS 1985 Proceedings. 4.
  http://aisel.aisnet.org/icis1985/4
