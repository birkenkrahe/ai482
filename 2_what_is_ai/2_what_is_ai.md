
# Table of Contents

-   [What're you going to learn?](#org8c59f4d)
-   [What is intelligence?](#org622236b)
    -   [Search patterns](#orgb6e2ff3)
    -   [Group work](#org5905eb2)
-   [Different approaches to AI](#orgb197369)
    -   [Fields of systematic inquiry](#org6ed4ff5)
    -   [Fundamental questions](#orge4c7878)
    -   [Four approaches](#org796d690)
        -   [Four scenarios](#org026b3ba)
        -   [Acting humanly ("Turing test" approach)](#org4e75988)
        -   [Thinking humanly ("cognitive modeling" approach)](#org5d53495)
        -   [Thinking rationally ("laws of thought" approach)](#orgcc55c9d)
        -   [Acting rationally ("rational agent" approach)](#orgacfdfd8)
-   [Major issues](#org6d367cc)
    -   [Bounded rationality](#org828c479)
    -   [Value alignment](#orgb0f6cf7)
    -   [Group work](#orga8315bd)
-   [Asimov's robot laws](#orgb027a1b)
    -   [Which appoach fits these laws?](#orgbf36cea)
-   [What's next?](#org2503c7b)
-   [Any questions?](#org816ca5b)
-   [References](#org795a489)



<a id="org8c59f4d"></a>

# What're you going to learn?

-   What is intelligence?
-   Different approaches to AI
-   The standard model of AI
-   Bounded rationality
-   The Value alignment problem
-   What's next?


<a id="org622236b"></a>

# What is intelligence?

![img](./img/intelligence.gif)


<a id="orgb6e2ff3"></a>

## Search patterns

![img](./img/googletrends.png)


<a id="org5905eb2"></a>

## Group work

![img](./img/groupwork.gif)

-   Get together in groups of 2-3
-   Define INTELLIGENCE (5')
-   Define ARTIFICIAL INTELLIGENCE (5')
-   Briefly present your results (10')


<a id="orgb197369"></a>

# Different approaches to AI

![img](./img/fields.gif)

Which fields of inquiry (= disciplines) to use?


<a id="org6ed4ff5"></a>

## Fields of systematic inquiry

![img](./img/fields.gif)

-   Language
-   Philosophy
-   Science
-   History


<a id="orge4c7878"></a>

## Fundamental questions

![img](./img/humanmachine.jpg)

-   Should we focus on humans?
-   Should we focus on machines?


<a id="org796d690"></a>

## Four approaches

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">**THOUGHT / LOGIC**</th>
<th scope="col" class="org-left">**BEHAVIOR / ACTION**</th>
</tr>
</thead>

<tbody>
<tr>
<td class="org-left">**HUMANITY**</td>
<td class="org-left">*Cognitive modeling*</td>
<td class="org-left">*Turing Test*</td>
</tr>


<tr>
<td class="org-left">**RATIONALITY**</td>
<td class="org-left">*Laws of Thought*</td>
<td class="org-left">*Rational Agents*</td>
</tr>
</tbody>
</table>


<a id="org026b3ba"></a>

### Four scenarios

![img](./img/approaches1.png)


<a id="org4e75988"></a>

### Acting humanly ("Turing test" approach)

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Natural language processing</td>
</tr>


<tr>
<td class="org-left">Knowledge representation</td>
</tr>


<tr>
<td class="org-left">Automated reasoning</td>
</tr>


<tr>
<td class="org-left">Machine learning</td>
</tr>


<tr>
<td class="org-left">Computer vision</td>
</tr>


<tr>
<td class="org-left">Robotics</td>
</tr>
</tbody>
</table>


<a id="org5d53495"></a>

### Thinking humanly ("cognitive modeling" approach)

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Introspection</td>
</tr>


<tr>
<td class="org-left">Psychological experiments</td>
</tr>


<tr>
<td class="org-left">Brain imaging</td>
</tr>


<tr>
<td class="org-left">Cognitive science</td>
</tr>


<tr>
<td class="org-left">Algorithms</td>
</tr>
</tbody>
</table>


<a id="orgcc55c9d"></a>

### Thinking rationally ("laws of thought" approach)

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Syllogistic reasoning</td>
</tr>


<tr>
<td class="org-left">Logic</td>
</tr>


<tr>
<td class="org-left">Expert systems</td>
</tr>


<tr>
<td class="org-left">Uncertainty</td>
</tr>


<tr>
<td class="org-left">Probability</td>
</tr>
</tbody>
</table>


<a id="orgacfdfd8"></a>

### Acting rationally ("rational agent" approach)

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Combination approach</td>
</tr>


<tr>
<td class="org-left">Constructivist</td>
</tr>


<tr>
<td class="org-left">Doing the right thing</td>
</tr>


<tr>
<td class="org-left">Standard model</td>
</tr>


<tr>
<td class="org-left">Control theory</td>
</tr>
</tbody>
</table>


<a id="org6d367cc"></a>

# Major issues

![img](./img/issues.gif)

-   Bounded Rationality
-   Value alignment problem


<a id="org828c479"></a>

## Bounded rationality

![img](./img/bakopoulos.png)

Image: [Bakopoulos, 1985](#org4339862)

> AIMA: "For perfect rationality, the computational demands are just
> too high."


<a id="orgb0f6cf7"></a>

## Value alignment

![img](./img/mechanicalturk.png)

Image: [The Mechanical Turk](https://www.amazon.com/Turk-Famous-Eighteenth-Century-Chess-Playing-Machine/dp/B000HWZ28Q)

> AIMA: "The values or objectives put into the machine must be
> aligned with those of the human."


<a id="orga8315bd"></a>

## Group work

![img](./img/groupwork.gif)

-   Get together in groups of 2-3
-   Each group covers one approach
-   List pros and cons of your approach
-   Put your results [on the Kanban board](https://ideaboardz.com/for/AI%20approaches%20pros%20&amp;%20cons/4063343)


<a id="orgb027a1b"></a>

# [Asimov's robot laws](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics)

![img](./img/asimov.jpg)

Image: cover of "I, Robot" by Isaac Asimov (1940)


<a id="orgbf36cea"></a>

## Which appoach fits these laws?

1.  A robot may not injure a human being or, through inaction, allow
    a human being to come to harm.
2.  A robot must obey the orders given it by human beings except
    where such orders would conflict with the First Law.
3.  A robot must protect its own existence as long as such
    protection does not conflict with the First or Second Law.


<a id="org2503c7b"></a>

# What's next?

![img](./img/river.gif)

-   Scientific foundations of AI


<a id="org816ca5b"></a>

# Any questions?

![img](./img/thankyou.gif)

[This presentation is available online.](https://github.com/birkenkrahe/ai482/tree/main/2_what_is_ai)


<a id="org795a489"></a>

# References

<a id="org4339862"></a> Bakopoulos, J. Yannis, "Toward a More Precise
Concept of Information Technology" (1985). ICIS 1985 Proceedings. 4.
<http://aisel.aisnet.org/icis1985/4>

