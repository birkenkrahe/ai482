#+TITLE:Intelligent Agents
#+AUTHOR: Marcus Birkenkrahe
#+Subtitle: Seminar on Artificial Intelligence
#+OPTIONS: toc:nil num:nil ^:nil
#+INFOJS_OPT: :view:info
* AI applications that might change the world[fn:4]

  | No | AI APPLICATION        | TYPE                 | AI FIELD                    |
  |----+-----------------------+----------------------+-----------------------------|
  |  1 | Alexa/Siri/[[https://mycroft.ai/][MyCroft]]    | Conversational agent | Natural Language Processing |
  |  2 | Autonomous vehicles   | Driving agent        | Pattern recognition         |
  |  3 | Autonomous drones     | Delivery agent       | Pattern recognition         |
  |----+-----------------------+----------------------+-----------------------------|
  |  4 | [[https://neuralink.com/][Neuralink]]             | Brain support agent  | Neuroscience                |
  |  5 | [[https://about.fb.com/news/2021/09/introducing-ray-ban-stories-smart-glasses/][Facebook Glass]]        | Social media agent   | Pattern recognition         |
  |  6 | [[https://www.jarvis.ai/][Automatic writing]]     | Writing agent        | Natural Language Processing |
  |  7 | [[https://openai.com/blog/openai-codex/][Automatic programming]] | Programming agent    | Natural Language Processing |
  |----+-----------------------+----------------------+-----------------------------|
* Agents and environments

  The purpose of this section is to bridge the gap between AI ideas
  and practice. To do this, we adopt the rational agent approach
  promoted in [[aima][AIMA]], and implemented in day-to-day AI systems, like
  robot vacuum cleaners that operate in the real world, or
  personalized shopping apps that operate in virtual commercial space.

  The abstract insights about rational agents are useful as an
  analytic framework, much like the distinction between supervised and
  unsupervised learning that we will look at next.

  These lecture notes corresponds to some of the chapter 2 content of
  the textbook ([[aima][Russell/Norvig]], 2021).

** Microworld-view

   The diagram shows a world- or space-oriented view of an agent and
   its environment. The microworld view is less intuitive than the
   process-view[fn:1] - think about your learning as a student: you
   don't dwell on the fact that you move from your personal into
   classroom space - and it would be difficult to see how to optimize
   this spatial movement (for learning). Instead, you operate - like
   an agent - with process steps (go to class, listen to lecture, do
   exercise, take notes etc.). It is easier to think of input and
   output and optimizing functions when following a process[fn:2].

   [[./img/agents.png]]

   Source: [[aima][AIMA]] - agent-world vs. environment view

** Rational agent success

   Success of a rational agent in this simple picture depends on:

   1) the performance measure that defines success
   2) the agent's knowledge of the environment
   3) the actions that the agent can perform
   4) the agent's percept sequence to date

** Example: robotic vacuum-cleaner

   The microworld of the vacuum cleaner has a known boundary, and it
   is divided in subspaces that the agent can navigate. In each
   subspace, there is an unspecified amount of dirt. The overall
   mission is to clean the space.

   [[./img/vacuum.png]]

   | ASPECT      | EXAMPLE             | FUNCTIONS |
   |-------------+---------------------+-----------|
   | Performance | Award cleanliness   | Maximize  |
   | Environment | Spatial dimensions  | Minimize  |
   | Actions     | Movements + sucking | Table     |
   | Perceptions | Location + dirt     | Table     |

   Can such a simple agent behave irrationally, too?[fn:3]

   It is worth noting that irrational behavior in humans can lead to
   unforeseen (and unforeseeable) innovations and optimizations, but
   also, of course, to destructive behavior. In fact, one could argue
   that controlled irrationality is the core of creative behavior and
   originality.

** Definition of a rational agent

   #+begin_quote
   "For each possible percept sequence, a rational agent should select
   an action that is expected to maximize its performance measure,
   given the evidence provided by the percept sequence and whatever
   built-in knowledge the agent has." ([[aima][AIMA]])
   #+end_quote

** Process view

   The definition suggests a process-oriented description of the
   behavior of a rational agent. Such a process is shown in the BPMN
   diagram below.

   [[./img/agents_and_environments.png]]

* Task environments

  The greatest challenge for the agent is operating in a given
  environment. To design agents that can manage this challenge, we
  classify different types of environments.

** Current research

   Recent article: "Solving the Robot Off-Loading Problem": helping
   robots choose when to communicate with the cloud without latency or
   lost data issues ([[myers][Myers, 2021]]). 

   [[./img/drone.jpg]]

** PEAS example: automated taxi driver

   PEAS description for an automated taxi driver (transport agent):

   | PERFORMANCE MEASURE                                      | ENVIRONMENT                                             | ACTUATORS                                                   | SENSORS                                                                         |
   |----------------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------------+---------------------------------------------------------------------------------|
   | Safe, fast, legal, comfy trip, max profits, min accident | Roads, traffic, police, pedestrians, customers, weather | Steering, accelerator, brake, signal, horn, display, speech | Camera, radar, GPS, speedometer, engine, accelerometer, microphone, touchscreen |

   #+begin_quote
   "Virtual task environments (not in the physical world) can be as
   complex as real ones." ([[aima][AIMA]])
   #+end_quote

   *What do you think:* are AUGMENTED reality environments more, less,
   or equally complex?[fn:5]

** PEAS Challenge

   Identify PEAS elements for each of these agent types!

   * Medical diagnosis system
   * Satellite image analysis system
   * Part-picking robot
   * Refinery controller
   * Interactive English tutor

     [[[https://github.com/birkenkrahe/ai482/blob/main/5_ai_agents/img/challenge.png][Solution]], [[aima][AIMA]] table 2.5]

   All of these applications are present today in their respective
   industrial environments - health care, weather forecasting,
   factories, processing plants, and online learning platforms.

   Notice how when you think about actuators and sensors, you might
   automatically think of humanoid robots. However, the most efficient
   solution need not be humanoid (using human behavior, or physiology
   as a design blueprint) - except when humans are directly involved
   (health care, tutoring).

** Task environment properties

   The following properties is a taxonomy akin to a classification of
   insects or plants in nature: like these, the properties of agents
   refer to an evolving ecology. Also, they are used to explore and,
   in a way, define machine "intelligence".

   | PROPERTY          | OPTIONS                                              |
   |-------------------+------------------------------------------------------|
   | 1) Observability  | Fully, partially or unobservable[fn:6]               |
   | 2) Multiplicity   | single or multi-agent                                |
   | 3) Performativity | competitive or cooperative                           |
   | 4) Determinacy    | deterministic, non-deterministic or stochastic[fn:7] |
   | 5) Causality      | sequential or episodic                               |
   | 6) Stativity      | static or dynamic or semidynamic (score[fn:8])             |
   | 7) Temporality    | Discrete or continuous (state/time)                  |
   | 8) Physicality    | Known or unknown (laws)                              |
   | 9) Virtuality     | Virtual or real                                      |
   | 10) Locality      | Local or non-local                                   |

** Environment of the automated taxi driver
   Let's look at the automated taxi driver as an example:

   1) *Observability:* The environment is partially observable - for
      example, the agent cannot observe (or even find out directly)
      what other drivers are thinking.
   2) *Multiplicity:* The agent could treat another vehicle as an
      object or as an agent, too. Which relationship it is depends on
      the design of both vehicles involved: are any messages exchanged
      between them, or are there decisions where one agent's actions
      depend on the other? For example, if the taxi agent has a sensor
      that is directly fed data from the other vehicle, they'd form a
      two-agent system whose actions depend on one another.
   3) *Performativity:* this applies only to multi-agent systems. The
      taxi environment is both cooperative (with other non-taxi
      vehicles), and competitive (with other taxi drivers).
   4) *Determinacy:* the taxi environment is non-deterministic since
      the behavior of traffic cannot be predicted with
      certainty. Certain aspects (e.g. traffic density, weather
      conditions etc.) are stochastic and can be predicted with
      quantities attached to them ("10% probability of rain during
      this trip").
   5) *Causality:* short term, episodic actions, have long-term
      consequences for the taxi agent - the agent has to think ahead
      based on the past driving history and its past actions. For
      example, when it suddenly begins to rain, it has to adjust its
      breaking behavior.
   6) *Stativity:* the driving environment changes continuously,
      requiring agent responses ("The street is wet, what do you want
      to do?") and adjustments.
   7) *Temporality:* taxi driving is a continous state and a continous
      time problem - speed and location of the taxi change all the
      time, and they change smoothly, not abruptly. Driving actions
      are also continuous. Sensor input itself is discrete (digital
      signals) but is treated as continuous (distributions are
      smoothed). 
   8) *Physicality:* some of the environment is unknown e.g. when the
      journey starts. This means that the taxi driver needs to learn
      properties of its environment that cannot be hard-coded at the
      start of the journey.
   9) *Virtuality:* the automated taxi driver has both a virtual
      (testing and simulation) environment and a real
      environment. Performance, actuation and sensing are relevant for
      both environments but only the real environment is productive.
   10) *Locality:* the taxi driver deals with both local and non-local
       data, e.g. construction sites along the road are non-local,
       while road signs encountered during the drive are local. For
       the performance, local data are more relevant.

   #+begin_quote
   "The hardest [agent task environment] case is partially observable,
   multiagent, nondeterministic, sequential, dynamic, continous, and
   unknown."  ([[aima][AIMA]])
   #+end_quote

   Here are examples for some of these categories.

   [[./img/environments.png]]

   [Image source: [[aima][AIMA]] table 2.6]

** Systemic interpretation

   These properties define the agent as a system with a boundary
   between itself and the environment, with elements (actuators,
   sensors), and relationships between these elements. This means that
   the properties are not limited to AI systems - you could use them
   to describe other cybernetic systems, like heating systems.

** Homework

   Pick any of the online exercises for this chapter of [[aima][AIMA]] (ch. 2)
   and work out a solution, or sketch a path towards a solution
   (e.g. by describing what one might do, in which order), or sketch
   specific problems and issues for discussion, and present in class
   (for: Friday October 1).

* References

** Publications

   <<bee>> Bee Z (24 Jan 2021). Grammarly is Garbage, and Here's Why
   [Video]. [[https://youtu.be/Q5rB9jDbTPU][Online: YouTube.com]].

   Chen M et al (14 Jul 2021). Evaluating Large Language Models Trained
   on Code. Preprint: [[https://arxiv.org/abs/2107.03374][arxiv:2107.03374]].

   <<dorner>> Dörner D (1990). The logic of failure. In:
   Phil. Trans. R. Soc. Lond. B 327:463-473.

   Facebook (9 Sep 2021). Introducing Ray-Ban Stories: First-Generation
   Smart Glasses. [[https://about.fb.com/news/2021/09/introducing-ray-ban-stories-smart-glasses/][Online: fb.com.]]

   <<matloff>> Matloff N (2020). Probability and Statistics for Data
   Science: Math + R + Data. CRC Press.

   <<myers>> Myers A (8 Sept 2021). Solving the Robot Off-Loading
   Problem. [[https://hai.stanford.edu/news/solving-robot-loading-problem][Online: hai.stanford.edu]].

   <<reed>> Reed Floren (1 April 2021). Jarvis.ai How to Write Blog
   Posts in 10 Minutes with Conversion.AI [Video]. [[https://youtu.be/z5_3S5nKfWQ?t=540][Online: youtube.com]].

   <<aima>> Russell S/Norvig P (2021). AI - A Modern Approach (4th
   ed). Pearson.

** Websites

   * mycroft.ai - MyCroft AI speech assistant
   * openai.com - OpenAI Codex for natural language translation to code
   * neuralink.com - brain interface software and hardware
   * jarvis.ai - blog writing software

* Whiteboards

  * [[https://drive.google.com/drive/folders/1cVty0VxQ2pU99cOk8LD-rJPsOi0pOm7Z?usp=sharing][September 20, 2021]]
  * September 22, 2021
  * September 24, 2021

* Footnotes

[fn:8]In a semidynamic environment, the enviroment itself does not
change but a performance score does - chess with a clock is an
example: when the clock is ticking, the performance time changes even
though the board is static.

[fn:7]AIMA distinguishes between non-deterministic (aka uncertain) and
stochastic (aka uncertain but with a quantifiable probability).

[fn:6]"Unobservable" in principle means that the agent has no sensors
in all task environments (it must have some data otherwise it could
not perform its optimization strategy).

[fn:5]The answer depends on the measure of "complexity". For the sake
of argument, one could assume the complexity of the real world to be
"1", and of a completely static virtual world "0". Alternatively, you
have to use a complexity measure that can be quantified and
e.g. implemented in a program like Dörner's in "The logic of failure"
([[dorner][Dörner, 1990]]).

[fn:4]1-3 came from course participants (see [[https://drive.google.com/drive/folders/1cVty0VxQ2pU99cOk8LD-rJPsOi0pOm7Z?usp=sharing][whiteboard, Sept 20]]), 4-7
are my personal opinion. "Automatic writing" includes AI-driven
spell-checking apps like Grammarly (beware - cp. [[bee][Bee 2021]], though the
[[https://www.grammarly.com/blog/engineering/][Grammarly engineering blog]] is quite interesting). Quote from a video
demonstrating jarvis.ai ([[reed][Reed Floren, 2021]]): "I've created a 1000 word
article in minutes on a topic that I really know nothing about."

[fn:3]The answer is yes: whenever the maximizing or minimizing
functions are not executed well, e.g. because of lack of environmental
knowledge, or because the performance measure is ill-defined, or
because of faulty sensor data. In the case of the Rumba: not moving
(action), or not sucking (action), not respecting the boundaries
(environment), stopping short of cleaning well because of faulty
rewarding (performance), etc.

[fn:2]You could also look at the job of learning in terms of incoming
or outgoing data, or different data formats. This would be closer to
computer processing and further from the human experience.

[fn:1]Much like in probability: these are usually introduced via state
spaces (e.g. the different combinations when rolling a dice). A better
way of thinking about probability is as a process of creating one
record after another - essentially an event log of stochastic
events. Cp. [[matloff][Matloff (2020)]].
